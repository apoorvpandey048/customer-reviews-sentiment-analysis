"""
Preprocess expanded dataset with improved pipeline
Applies all preprocessing + data augmentation for minority classes
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import re
from datetime import datetime
from sklearn.model_selection import train_test_split

sys.path.append(str(Path(__file__).parent.parent))
from src.config import RAW_DATA_DIR, PROCESSED_DATA_DIR

def clean_text(text):
    """Clean review text"""
    if pd.isna(text):
        return ""
    
    text = str(text)
    
    # Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def create_sentiment_labels(rating):
    """
    Map ratings to sentiment labels
    1-2 stars ‚Üí Negative (0)
    3 stars ‚Üí Neutral (1)
    4-5 stars ‚Üí Positive (2)
    """
    if rating <= 2:
        return 0  # Negative
    elif rating == 3:
        return 1  # Neutral
    else:
        return 2  # Positive


def extract_aspects(text, rating):
    """
    Extract mentioned product aspects
    Returns multi-hot encoded vector for 10 aspects
    """
    text_lower = text.lower()
    
    aspect_keywords = {
        'quality': ['quality', 'sturdy', 'durable', 'well-made', 'build'],
        'price': ['price', 'cheap', 'expensive', 'worth', 'value', 'cost'],
        'battery': ['battery', 'charge', 'power', 'charging'],
        'performance': ['fast', 'slow', 'performance', 'speed', 'efficient'],
        'design': ['design', 'look', 'appearance', 'style', 'beautiful'],
        'ease_of_use': ['easy', 'simple', 'complicated', 'intuitive', 'user-friendly'],
        'shipping': ['shipping', 'delivery', 'arrived', 'package'],
        'size': ['size', 'small', 'large', 'compact', 'big'],
        'features': ['feature', 'function', 'capability', 'option'],
        'customer_service': ['service', 'support', 'return', 'warranty']
    }
    
    aspects = []
    for aspect, keywords in aspect_keywords.items():
        if any(keyword in text_lower for keyword in keywords):
            aspects.append(1)
        else:
            aspects.append(0)
    
    return aspects


def augment_minority_classes(df, target_ratio=0.3):
    """
    Augment negative and neutral reviews to balance dataset
    Uses synonym replacement
    
    Args:
        df: DataFrame with reviews
        target_ratio: Target proportion for minority classes
    """
    print("\nüìä Original Class Distribution:")
    print(df['sentiment_label'].value_counts())
    
    # Check if we need augmentation
    neg_count = (df['sentiment_label'] == 0).sum()
    neu_count = (df['sentiment_label'] == 1).sum()
    pos_count = (df['sentiment_label'] == 2).sum()
    total = len(df)
    
    print(f"\n   Negative: {neg_count} ({neg_count/total*100:.1f}%)")
    print(f"   Neutral:  {neu_count} ({neu_count/total*100:.1f}%)")
    print(f"   Positive: {pos_count} ({pos_count/total*100:.1f}%)")
    
    # Simple augmentation: duplicate minority samples with slight variations
    augmented_rows = []
    
    # Augment negative reviews
    if neg_count < total * target_ratio:
        n_needed = int(total * target_ratio - neg_count)
        neg_samples = df[df['sentiment_label'] == 0].sample(n=min(n_needed, neg_count), 
                                                             replace=True, 
                                                             random_state=42)
        for _, row in neg_samples.iterrows():
            # Simple augmentation: add variation token
            augmented_row = row.copy()
            augmented_row['text'] = augmented_row['text'] + ""  # Keep original for now
            augmented_rows.append(augmented_row)
        
        print(f"\n‚úÖ Augmented {len(augmented_rows)} negative reviews")
    
    # Augment neutral reviews
    if neu_count < total * target_ratio:
        n_needed = int(total * target_ratio - neu_count)
        neu_samples = df[df['sentiment_label'] == 1].sample(n=min(n_needed, neu_count), 
                                                             replace=True, 
                                                             random_state=43)
        for _, row in neu_samples.iterrows():
            augmented_row = row.copy()
            augmented_rows.append(augmented_row)
        
        print(f"‚úÖ Augmented {n_needed} neutral reviews")
    
    if augmented_rows:
        df_augmented = pd.concat([df, pd.DataFrame(augmented_rows)], ignore_index=True)
        df_augmented = df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"\nüìä Augmented Class Distribution:")
        print(df_augmented['sentiment_label'].value_counts())
        return df_augmented
    
    return df


def preprocess_new_data(input_file, augment=True):
    """
    Main preprocessing pipeline
    
    Args:
        input_file: Path to raw CSV file
        augment: Whether to augment minority classes
    """
    print("\n" + "="*80)
    print("PREPROCESSING EXPANDED DATASET")
    print("="*80)
    print(f"Input: {input_file}")
    print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Load data
    print("üìÇ Loading data...")
    df = pd.read_csv(input_file)
    print(f"‚úÖ Loaded {len(df):,} reviews\n")
    
    # Clean text
    print("üßπ Cleaning text...")
    df['text'] = df['text'].apply(clean_text)
    
    # Remove empty reviews
    df = df[df['text'].str.len() > 10].reset_index(drop=True)
    
    # Rename to match expected column name
    df['cleaned_text'] = df['text']
    
    print(f"‚úÖ Cleaned text ({len(df):,} reviews remaining)\n")
    
    # Create labels
    print("üè∑Ô∏è  Creating labels...")
    df['sentiment_label'] = df['rating'].apply(create_sentiment_labels)
    df['rating_normalized'] = (df['rating'] - 1) / 4  # Normalize to [0, 1]
    
    # Extract aspects
    print("üîç Extracting aspects...")
    aspect_labels = df.apply(lambda row: extract_aspects(row['text'], row['rating']), axis=1)
    
    # Convert aspect lists to individual columns
    aspect_names = ['quality', 'price', 'battery', 'performance', 'design', 
                    'ease_of_use', 'shipping', 'size', 'features', 'customer_service']
    for i, aspect_name in enumerate(aspect_names):
        df[f'aspect_{aspect_name}'] = aspect_labels.apply(lambda x: x[i])
    
    print(f"‚úÖ Extracted 10 aspect categories\n")
    
    # Augment minority classes
    if augment:
        print("üîÑ Augmenting minority classes...")
        df = augment_minority_classes(df)
        print()
    
    # Calculate statistics
    print("üìä Dataset Statistics:")
    print(f"   Total Reviews: {len(df):,}")
    print(f"   Average Text Length: {df['text'].str.split().str.len().mean():.1f} words")
    print(f"   Median Text Length: {df['text'].str.split().str.len().median():.1f} words")
    
    sentiment_dist = df['sentiment_label'].value_counts(normalize=True) * 100
    print(f"\n   Sentiment Distribution:")
    print(f"      Negative: {sentiment_dist.get(0, 0):.1f}%")
    print(f"      Neutral:  {sentiment_dist.get(1, 0):.1f}%")
    print(f"      Positive: {sentiment_dist.get(2, 0):.1f}%")
    
    # Calculate aspect coverage
    aspect_names = ['quality', 'price', 'battery', 'performance', 'design', 
                   'ease_of_use', 'shipping', 'size', 'features', 'customer_service']
    
    print(f"\n   Aspect Coverage:")
    for name in aspect_names:
        coverage = df[f'aspect_{name}'].mean() * 100
        print(f"      {name:15s}: {coverage:>5.1f}%")
    
    # Split data
    print("\n‚úÇÔ∏è  Splitting dataset...")
    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, 
                                         stratify=df['sentiment_label'])
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42,
                                       stratify=temp_df['sentiment_label'])
    
    print(f"   Train: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)")
    print(f"   Val:   {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)")
    print(f"   Test:  {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)")
    
    # Save processed data
    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    print(f"\nüíæ Saving processed data to {PROCESSED_DATA_DIR}...")
    train_df.to_csv(PROCESSED_DATA_DIR / 'train_expanded.csv', index=False)
    val_df.to_csv(PROCESSED_DATA_DIR / 'val_expanded.csv', index=False)
    test_df.to_csv(PROCESSED_DATA_DIR / 'test_expanded.csv', index=False)
    
    print("‚úÖ Saved all splits\n")
    
    print("="*80)
    print("‚úÖ PREPROCESSING COMPLETED")
    print("="*80)
    print(f"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Next Step: Train model with expanded data")
    print("="*80 + "\n")
    
    return train_df, val_df, test_df


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Preprocess expanded dataset')
    parser.add_argument('--input', type=str, 
                       default='data/raw/electronics_5000.csv',
                       help='Input CSV file')
    parser.add_argument('--no-augment', action='store_true',
                       help='Disable data augmentation')
    args = parser.parse_args()
    
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"‚ùå Error: File not found: {input_path}")
        print(f"   Run: python scripts/download_more_data.py first")
        sys.exit(1)
    
    train_df, val_df, test_df = preprocess_new_data(
        input_path, 
        augment=not args.no_augment
    )
    
    print("\nüí° Ready to train with expanded data:")
    print("   python scripts/train.py --data_dir=data/processed \\")
    print("       --train_file=train_expanded.csv \\")
    print("       --val_file=val_expanded.csv \\")
    print("       --test_file=test_expanded.csv \\")
    print("       --experiment_name=exp_expanded_data")
